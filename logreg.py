# -*- coding: utf-8 -*-
"""COMP9417_Logreg_Submission

Automatically generated by Colab.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize, LabelBinarizer
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.metrics import (
    f1_score,
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
    make_scorer
)

DATA_FOLDER = "drive/MyDrive/COMP9417/"

# Source distribution, q(X, y)
X = pd.read_csv(DATA_FOLDER + "X_train.csv")
y = pd.read_csv(DATA_FOLDER + "y_train.csv").squeeze()

CLASSES = np.unique(y)

# Note, that whilst it is labelled X_val, this is actually a TEST set.
# Validation is done using KFoldCV with k=4 during hyperparameter tuning.
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)

means = X_train.mean(axis=0)
stds = X_train.std(axis=0)

lower_bounds = means - 2.5 * stds
upper_bounds = means + 2.5 * stds

X_train = X_train.clip(lower=lower_bounds, upper=upper_bounds, axis=1)

X_val = X_val.clip(lower=lower_bounds, upper=upper_bounds, axis=1)

# Log loss, weighted up the number of samples in each class (lower is better)
def weighted_log_loss(y_true, y_pred_proba):
    epsilon = 1e-15
    y_pred_proba = np.clip(y_pred_proba, epsilon, 1 - epsilon)
    y_true_ohe = label_binarize(y_true, classes=CLASSES)

    class_counts = np.sum(y_true_ohe, axis=0)

    # Weight the classes by the inverse frequencies
    # Classes with no samples should be weighted as 0
    # See: https://stackoverflow.com/a/37977222/21453336
    class_weights = np.divide(
        1.0,
        class_counts,
        out=np.zeros(class_counts.shape, dtype=float),
        where=class_counts!=0
    )

    class_weights /= np.sum(class_weights)

    sample_weights = np.sum(y_true_ohe * class_weights, axis=1)

    loss = -np.mean(sample_weights * np.sum(y_true_ohe * np.log(y_pred_proba), axis=1))
    return loss

# Negative weighted log loss (higher is better)
neg_wll = make_scorer(weighted_log_loss, response_method="predict_proba", greater_is_better=False)

"""Initial tests showed that PCA leads to a drop in performance regardless of the number of components selected, likely due to the moderate to low level of collinearity in the features. The training time was also fast enough to not necessitate dimensionality reduction.

An initial grid search over a wide range of values for `C`, `max_iter`, and `tol` revealed the scikit-learn default values for these parameters were reasonable, aside from the `C` value. By default, `C=1.0`, however it was found that the model performed better with values on the order of $10^{-4}$ to $10^{-3}$ (with smaller values -- i.e., more regularisation -- prefered for larger `max_iter`).

We will thus use scikit-learn's `LogisticRegressionCV` to perform grid search for the value of `C`. By default, it searches 10 `C` values evenly spaced on a log scale between `10^{-4}` and `10^4`. As training was extremely fast (5 seconds for one `C`), we tested more `C` values and narrowed the range based on the prior tests.
"""

lr_cv = LogisticRegressionCV(
    Cs=np.logspace(-5, 1, 30),
    class_weight='balanced',
    scoring=neg_wll,
    cv=4,
    n_jobs=-1,
)

lr_cv_pipe = Pipeline(steps=[
    ("scaler", StandardScaler()),
    ("classifier", lr_cv),
])

lr_cv_pipe.fit(X_train, y_train)

all_class_scores = np.array([lr_cv.scores_[cls] for cls in lr_cv.scores_])
mean_losses = all_class_scores.mean(axis=(0, 1))

best_index = np.argmax(mean_losses)

best_C = lr_cv.Cs_[best_index]
best_loss = mean_losses[best_index]

print(f"Best C: {best_C}")
print(f"Best avg CV loss: {best_loss}")

best_model = Pipeline(steps=[
    ("scaler", StandardScaler()),
    ("classifier", LogisticRegression(
        C=0.004893900918477494,
        class_weight='balanced',
        solver='lbfgs',
        multi_class='multinomial',
        max_iter=1000
    ))
])

best_model.fit(X_train, y_train)

y_pred_val = best_model.predict(X_val)
y_proba_val = best_model.predict_proba(X_val)

wll_val = weighted_log_loss(y_val, y_proba_val)
print(wll_val)

from matplotlib.colors import LogNorm

def plot_metrics(y_true, y_pred, title_info: str):
    fig = plt.figure(layout="constrained", figsize=(17, 7))

    fig_left, fig_right = fig.subfigures(1, 2) # (to add spacing, use e.g. wspace=0.1)
    ax1, ax2 = fig_left.subplots(1, 2, sharey=True, width_ratios=[3, 1])

    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)
    report_df = pd.DataFrame(report).transpose()

    # Left side: Main metrics heatmap (precision, recall, f1-score)
    sns.heatmap(
        report_df.iloc[:, :-1],
        annot=True,
        cmap="Blues",
        vmin=0,
        vmax=1,
        cbar=False,
        ax=ax1,
    )

    # Middle: Support heatmap (number of samples in each class)
    sns.heatmap(
        report_df.iloc[:, [-1]],
        annot=True,
        cmap="Greens",
        cbar=False,
        ax=ax2,
        fmt=".0f",
        norm=LogNorm(),
    )
    ax2.tick_params(axis='y', which='both', left=False) # turn off major & minor ticks on the left
    fig_left.suptitle("Classification Report")

    # Right side: Confusion matrix
    ax3 = fig_right.subplots()
    cm = confusion_matrix(
        y_true,
        y_pred,
        normalize=None, # normalise over the true conditions (rows)
    )

    disp = ConfusionMatrixDisplay(cm)

    disp.plot(
        ax=ax3,
        cmap=sns.color_palette("rocket_r", as_cmap=True),
        values_format=".0f",
        text_kw={"fontsize": "small"},
    )

    labels = np.union1d(y_true, y_pred)
    disp.ax_.set_xticklabels(labels)
    disp.ax_.set_yticklabels(labels)

    fig_right.suptitle("Confusion Matrix (Percentages of True Labels)")
    fig.suptitle(f"Model Performance ({title_info})", fontsize="x-large")
    plt.show()

plot_metrics(y_val, y_pred_val, "LogisticRegressionCV, Test Set")