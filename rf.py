# -*- coding: utf-8 -*-
"""rf.ipynb

Automatically generated by Colab.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report, make_scorer
from sklearn.preprocessing import LabelBinarizer
from sklearn.pipeline import Pipeline
from sklearn.inspection import permutation_importance

from imblearn.ensemble import BalancedRandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.inspection import permutation_importance
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestClassifier

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.datasets import make_classification
from collections import Counter

from collections import Counter

import itertools
import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone
from imblearn.over_sampling import SMOTE

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize, LabelBinarizer
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.metrics import (
    f1_score,
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
    make_scorer
)

X = pd.read_csv("X_train.csv")
y = pd.read_csv("y_train.csv").squeeze()

# these are the negative importance columns from permutation importance, so we drop them
cols_to_remove = ['1', '12', '19', '24', '28', '29', '34', '38', '40', '44', '48', '53', '56', '57', '58', '63', '68', '75', '77', '78', '79', '100', '104', '113', '115', '126', '135', '144', '147', '152', '155', '156', '163', '166', '167', '176', '188', '189', '190', '204', '206', '207', '210', '215', '231', '234', '239', '248', '251', '253', '257', '258', '269', '278', '288', '291', '292', '296', '297', '298', '299']

print(len(cols_to_remove))
X_copy, y_copy = X.copy(), y.copy()

X = X.drop(columns=cols_to_remove, errors='ignore')

def weighted_log_loss(y_true, y_pred_proba, classes_):
    lb = LabelBinarizer()
    lb.fit(classes_)
    y_true_onehot = lb.transform(y_true)

    class_counts = np.sum(y_true_onehot, axis=0)
    class_weights = 1.0 / class_counts
    class_weights /= np.sum(class_weights)

    sample_weights = np.sum(y_true_onehot * class_weights, axis=1)
    loss = -np.mean(sample_weights * np.sum(y_true_onehot * np.log(y_pred_proba + 1e-15), axis=1))
    return loss

def weighted_log_loss_scorer(estimator, X, y):
    y_pred_proba = estimator.predict_proba(X)
    return -weighted_log_loss(y, y_pred_proba, estimator.classes_)

X, X_test, y, y_test = train_test_split(
    X, y, test_size=0.25, stratify=y, random_state=0
)

# minimum samples is 200 for any minority class
def create_sampling_strategy(y_train, min_samples=200):
    class_counts = Counter(y_train)
    strategy = {}

    for label, count in class_counts.items():
        if count < min_samples:
            strategy[label] = min_samples
        else:
            strategy[label] = count
    return strategy

def SMOTEGridSearchCV(estimator, param_grid, X, y, sample_size, cv=4):

    param_names = list(param_grid.keys())

    best_score = -np.inf
    best_params = None
    best_estimator = None

    # all combinations of parameters in the grid
    for p in list(itertools.product(*param_grid.values())):
        params = dict(zip(param_names, p))

        # we need to clone so we don't override it
        model = clone(estimator)
        model.set_params(**params)

        scores = []

        kf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=4)

        for train_idx, val_idx in kf.split(X, y):
            # training and validation sets
            X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]
            X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]

            # we use our custom sampling strategy (min 200 samples for minorites)
            s_strategy = create_sampling_strategy(y_train, sample_size)
            smote = SMOTE(sampling_strategy=s_strategy, k_neighbors=2, random_state=4)

            X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)

            model_fold = clone(model)
            model_fold.fit(X_train_sm, y_train_sm)

            score = weighted_log_loss_scorer(model_fold, X_val, y_val)
            scores.append(score)

        avg_score = np.mean(scores)

        print(f"avg score:{avg_score:.6f}\nparameters: {params} ")

        # update best parameter
        if avg_score > best_score:
            best_score = avg_score
            best_params = params
            best_estimator = clone(estimator)
            best_estimator.set_params(**params)

            s_strategy = create_sampling_strategy(y, sample_size)
            smote = SMOTE(sampling_strategy=s_strategy, k_neighbors=3, random_state=4)

            X_sm, y_sm = smote.fit_resample(X, y)
            best_estimator.fit(X_sm, y_sm)

    return best_estimator, best_params, best_score

rf = BalancedRandomForestClassifier(
    random_state=0,
    n_jobs=-1,
    class_weight='balanced_subsample',
    sampling_strategy="not majority",
    max_samples=None,
    n_estimators=500
)

param_grid = {
    'max_depth': [7, 10],
    'min_samples_split': [2, 3],
    'min_samples_leaf': [1, 2],
    'criterion': ['entropy'],
    'max_features': ['sqrt', 10, 30],
    'class_weight': ['balanced_subsample']
}


best_model, best_params, best_score = SMOTEGridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    X=X,
    y=y,
    cv=4,
    sample_size=200
)

print("\nBest Parameters:", best_params)
print("Best Cross-Validated Score:", best_score)

"""Best Parameters: {'n_estimators': 500, 'max_depth': 7, 'min_samples_split': 3, 'min_samples_leaf': 1, 'criterion': 'entropy', 'max_features': 'sqrt', 'max_samples': 4000, 'class_weight': 'balanced_subsample'}
Best Cross-Validated Score: -0.00711038780506428
"""

final_sampling_strategy = create_sampling_strategy(y, min_samples=200)
smote = SMOTE(sampling_strategy=final_sampling_strategy,
              k_neighbors=3,
              random_state=0)
X_smote, y_smote = smote.fit_resample(X, y)

final_model = BalancedRandomForestClassifier(random_state=0,
                                     n_estimators=500,
                                     max_depth=7,
                                     min_samples_split=3,
                                     min_samples_leaf=1,
                                     n_jobs=-1,
                                     criterion='entropy',
                                     sampling_strategy="not majority",
                                     max_features='sqrt',
                                     class_weight='balanced_subsample')
final_model.fit(X_smote, y_smote)

y_pred = final_model.predict(X_test)
y_pred_proba = final_model.predict_proba(X_test)

log_loss_value = weighted_log_loss(y_test, y_pred_proba, final_model.classes_)
print(f"Weighted Log Loss (Test Set): {log_loss_value:.6f}")

from matplotlib.colors import LogNorm

def plot_metrics(y_true, y_pred, title_info: str):
    fig = plt.figure(layout="constrained", figsize=(17, 7))

    fig_left, fig_right = fig.subfigures(1, 2) # (to add spacing, use e.g. wspace=0.1)
    ax1, ax2 = fig_left.subplots(1, 2, sharey=True, width_ratios=[3, 1])

    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)
    report_df = pd.DataFrame(report).transpose()

    # Left side: Main metrics heatmap (precision, recall, f1-score)
    sns.heatmap(
        report_df.iloc[:, :-1],
        annot=True,
        cmap="Blues",
        vmin=0,
        vmax=1,
        cbar=False,
        ax=ax1,
    )

    # Middle: Support heatmap (number of samples in each class)
    sns.heatmap(
        report_df.iloc[:, [-1]],
        annot=True,
        cmap="Greens",
        cbar=False,
        ax=ax2,
        fmt=".0f",
        norm=LogNorm(),
    )
    ax2.tick_params(axis='y', which='both', left=False) # turn off major & minor ticks on the left
    fig_left.suptitle("Classification Report")

    # Right side: Confusion matrix
    ax3 = fig_right.subplots()
    cm = confusion_matrix(
        y_true,
        y_pred,
        normalize=None, # normalise over the true conditions (rows)
    )

    disp = ConfusionMatrixDisplay(cm)

    disp.plot(
        ax=ax3,
        cmap=sns.color_palette("rocket_r", as_cmap=True),
        values_format=".0f",
        text_kw={"fontsize": "small"},
    )

    labels = np.union1d(y_true, y_pred)
    disp.ax_.set_xticklabels(labels)
    disp.ax_.set_yticklabels(labels)

    fig_right.suptitle("Confusion Matrix (Percentages of True Labels)")
    fig.suptitle(f"Model Performance ({title_info})", fontsize="x-large")
    plt.show()

plot_metrics(y_test, y_pred, "LogisticRegressionCV, Test Set")