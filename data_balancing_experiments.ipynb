{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df4e335c",
      "metadata": {
        "id": "df4e335c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
        "from imblearn.combine import SMOTETomek\n",
        "\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y7JKJYaStKu_",
      "metadata": {
        "id": "y7JKJYaStKu_"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "518a0159",
      "metadata": {
        "id": "518a0159"
      },
      "outputs": [],
      "source": [
        "X = pd.read_csv(\"X_train.csv\").values\n",
        "y = pd.read_csv(\"y_train.csv\").squeeze()\n",
        "\n",
        "# TODO: pipelining (https://scikit-learn.org/stable/modules/compose.html)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# classes_to_remove = [5, 6, 14, 17]\n",
        "# mask = ~np.isin(y, classes_to_remove)\n",
        "# X = X[mask]\n",
        "# y = y[mask]\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff537458",
      "metadata": {
        "id": "ff537458"
      },
      "outputs": [],
      "source": [
        "classes = np.unique(y)\n",
        "y_one_hot = label_binarize(y, classes=classes) # TODO: investigate TransformedTargetRegressor? https://scikit-learn.org/stable/modules/compose.html#transforming-target-in-regression\n",
        "class_counts = np.sum(y_one_hot, axis=0)\n",
        "class_weights = 1.0 / class_counts\n",
        "class_weights /= np.sum(class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3df85d17",
      "metadata": {
        "id": "3df85d17"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f204a054",
      "metadata": {
        "id": "f204a054"
      },
      "outputs": [],
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "Experiment = namedtuple(\"Experiment\", [\"resampler\", \"use_weights\", \"model_type\"])\n",
        "\n",
        "experiments = {\n",
        "    # AdaBoost\n",
        "    'ADA - Baseline': Experiment(None, False, 'ada'),\n",
        "    'ADA - SMOTE': Experiment(SMOTE(random_state=4, k_neighbors=3), False, 'ada'),\n",
        "    'ADA - ADASYN': Experiment(ADASYN(random_state=42, n_neighbors=3), False, 'ada'),\n",
        "    'ADA - Undersampling': Experiment(RandomUnderSampler(random_state=42), False, 'ada'),\n",
        "    'ADA - Class Weighting': Experiment(None, True, 'ada'),\n",
        "    'ADA - SMOTE + Tomek': Experiment(SMOTETomek(smote=SMOTE(k_neighbors=3),\n",
        "                                                 tomek=TomekLinks(sampling_strategy='majority'),\n",
        "                                                 random_state=42), False, 'ada'),\n",
        "\n",
        "    # HistGradientBoosting\n",
        "    'HGB - Baseline': Experiment(None, False, 'hgb'),\n",
        "    'HGB - SMOTE': Experiment(SMOTE(random_state=4, k_neighbors=3), False, 'hgb'),\n",
        "    'HGB - ADASYN': Experiment(ADASYN(random_state=42, n_neighbors=3), False, 'hgb'),\n",
        "    'HGB - Undersampling': Experiment(RandomUnderSampler(random_state=42), False, 'hgb'),\n",
        "    'HGB - Class Weighting': Experiment(None, True, 'hgb'),\n",
        "    'HGB - SMOTE + Tomek': Experiment(SMOTETomek(smote=SMOTE(k_neighbors=3),\n",
        "                                                 tomek=TomekLinks(sampling_strategy='majority'),\n",
        "                                                 random_state=42), False, 'hgb'),\n",
        "\n",
        "    # Logistic Regression\n",
        "    'LR - Baseline': Experiment(None, False, 'lr'),\n",
        "    'LR - SMOTE': Experiment(SMOTE(random_state=4, k_neighbors=3), False, 'lr'),\n",
        "    'LR - ADASYN': Experiment(ADASYN(random_state=42, n_neighbors=3), False, 'lr'),\n",
        "    'LR - Undersampling': Experiment(RandomUnderSampler(random_state=42), False, 'lr'),\n",
        "    'LR - Class Weighting': Experiment(None, True, 'lr'),\n",
        "    'LR - SMOTE + Tomek': Experiment(SMOTETomek(smote=SMOTE(k_neighbors=3),\n",
        "                                                tomek=TomekLinks(sampling_strategy='majority'),\n",
        "                                                random_state=42), False, 'lr'),\n",
        "\n",
        "    # Random Forest\n",
        "    'RF - Baseline': Experiment(None, False, 'rf'),\n",
        "    'RF - SMOTE': Experiment(SMOTE(random_state=4, k_neighbors=3), False, 'rf'),\n",
        "    'RF - ADASYN': Experiment(ADASYN(random_state=42, n_neighbors=3), False, 'rf'),\n",
        "    'RF - Undersampling': Experiment(RandomUnderSampler(random_state=42), False, 'rf'),\n",
        "    'RF - Class Weighting': Experiment(None, True, 'rf'),\n",
        "    'RF - SMOTE + Tomek': Experiment(SMOTETomek(smote=SMOTE(k_neighbors=3),\n",
        "                                                tomek=TomekLinks(sampling_strategy='majority'),\n",
        "                                                random_state=42), False, 'rf'),\n",
        "\n",
        "    # Support Vector Machine\n",
        "    'SVM - Baseline': Experiment(None, False, 'svm'),\n",
        "    'SVM - SMOTE': Experiment(SMOTE(random_state=4, k_neighbors=3), False, 'svm'),\n",
        "    'SVM - ADASYN': Experiment(ADASYN(random_state=42, n_neighbors=3), False, 'svm'),\n",
        "    'SVM - Undersampling': Experiment(RandomUnderSampler(random_state=42), False, 'svm'),\n",
        "    'SVM - Class Weighting': Experiment(None, True, 'svm'),\n",
        "    'SVM - SMOTE + Tomek': Experiment(SMOTETomek(smote=SMOTE(k_neighbors=3),\n",
        "                                                 tomek=TomekLinks(sampling_strategy='majority'),\n",
        "                                                 random_state=42), False, 'svm'),\n",
        "}\n",
        "\n",
        "results = {}\n",
        "predictions = {}\n",
        "probabilities = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c48744ce",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "c48744ce",
        "outputId": "a91d6ff7-7d19-4e46-c86b-84a8b7bec3a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training: ADA - Baseline\n",
            "Training: ADA - SMOTE\n",
            "Training: ADA - ADASYN\n",
            "Training: ADA - Undersampling\n",
            "Training: ADA - Class Weighting\n",
            "Training: ADA - SMOTE + Tomek\n",
            "Training: HGB - Baseline\n",
            "Training: HGB - SMOTE\n",
            "Training: HGB - ADASYN\n",
            "Training: HGB - Undersampling\n",
            "Training: HGB - Class Weighting\n",
            "Training: HGB - SMOTE + Tomek\n",
            "Training: LR - Baseline\n",
            "Training: LR - SMOTE\n",
            "Training: LR - ADASYN\n",
            "Training: LR - Undersampling\n",
            "Training: LR - Class Weighting\n",
            "Training: LR - SMOTE + Tomek\n",
            "Training: RF - Baseline\n",
            "Training: RF - SMOTE\n",
            "Training: RF - ADASYN\n",
            "Training: RF - Undersampling\n",
            "Training: RF - Class Weighting\n",
            "Training: RF - SMOTE + Tomek\n",
            "Training: SVM - Baseline\n",
            "Training: SVM - SMOTE\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "for name, config in experiments.items():\n",
        "    print(f\"Training: {name}\")\n",
        "    X_resampled, y_resampled = X_train, y_train\n",
        "\n",
        "    if config.resampler is not None:\n",
        "        X_resampled, y_resampled = config.resampler.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Initialize the model based on type and weighting\n",
        "    # TODO: hyperparameter optimisation? (Grid search).. + cross_validation?\n",
        "    if config.model_type == 'hgb':\n",
        "        model = HistGradientBoostingClassifier(\n",
        "            loss='log_loss',\n",
        "            class_weight='balanced' if config.use_weights else None\n",
        "        )\n",
        "\n",
        "    elif config.model_type == 'lr':\n",
        "        model = LogisticRegression(\n",
        "            max_iter=1000,\n",
        "            multi_class='multinomial',\n",
        "            solver='lbfgs',\n",
        "            class_weight='balanced' if config.use_weights else None\n",
        "        )\n",
        "\n",
        "    elif config.model_type == 'rf':\n",
        "        model = RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            random_state=42,\n",
        "            class_weight='balanced' if config.use_weights else None\n",
        "        )\n",
        "\n",
        "    elif config.model_type == 'svm':\n",
        "        model = SVC(\n",
        "            kernel='rbf',\n",
        "            probability=True,\n",
        "            random_state=42,\n",
        "            class_weight='balanced' if config.use_weights else None\n",
        "        )\n",
        "\n",
        "    elif config.model_type == 'ada':\n",
        "        base = LogisticRegression(\n",
        "            max_iter=1000,\n",
        "            multi_class='multinomial',\n",
        "            solver='lbfgs',\n",
        "            class_weight='balanced' if config.use_weights else None\n",
        "        )\n",
        "        model = AdaBoostClassifier(\n",
        "            estimator=base,\n",
        "            n_estimators=50,\n",
        "            algorithm='SAMME',\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {config.model_type}\")\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(X_resampled, y_resampled)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)\n",
        "\n",
        "    predictions[name] = y_pred\n",
        "    probabilities[name] = y_proba\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e600c405",
      "metadata": {
        "id": "e600c405"
      },
      "outputs": [],
      "source": [
        "def weighted_log_loss(y_true, y_pred_proba):\n",
        "    # One-hot encode y_true\n",
        "    ohe = OneHotEncoder(sparse_output=False, categories='auto', handle_unknown='ignore')\n",
        "    y_true = np.array(y_true).reshape(-1, 1)\n",
        "    y_true_bin = ohe.fit_transform(y_true) # TODO: check - binarised twice??\n",
        "\n",
        "    # Compute class weights (inverse frequency, normalized)\n",
        "    class_counts = np.sum(y_true_bin, axis=0)\n",
        "    class_weights = 1.0 / class_counts\n",
        "    class_weights /= np.sum(class_weights)\n",
        "\n",
        "    # Compute sample weights using the one-hot labels and class weights\n",
        "    sample_weights = np.sum(y_true_bin * class_weights, axis=1)\n",
        "\n",
        "    # Compute weighted log loss\n",
        "    eps = 1e-15  # To avoid log(0)\n",
        "    loss = -np.mean(sample_weights * np.sum(y_true_bin * np.log(np.clip(y_pred_proba, eps, 1)), axis=1))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "011f5979",
      "metadata": {
        "id": "011f5979"
      },
      "outputs": [],
      "source": [
        "print(\"\\nEvaluation Metrics for Each Method:\")\n",
        "for method in predictions:\n",
        "    y_pred = predictions[method]\n",
        "    y_proba = probabilities[method]\n",
        "\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    wll = weighted_log_loss(y_test, y_proba)\n",
        "\n",
        "    results[method] = (f1, wll)\n",
        "    print(f\"{method:25s} | Macro F1: {f1:.4f} | Weighted Log Loss: {wll:.4f}\")\n",
        "\n",
        "fig, axes = plt.subplots(len(predictions), 2, figsize=(16, 6 * len(predictions)))\n",
        "fig.tight_layout(pad=5.0)\n",
        "\n",
        "for idx, (method, y_pred_vals) in enumerate(predictions.items()):\n",
        "    report = classification_report(y_test, y_pred_vals, output_dict=True, zero_division=0)\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "    sns.heatmap(report_df.iloc[:-1, :-1], annot=True, fmt=\".2f\", cmap=\"Blues\", ax=axes[idx, 0])\n",
        "    axes[idx, 0].set_title(f\"{method} - Classification Report\")\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred_vals)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
        "    disp.plot(ax=axes[idx, 1], cmap=\"Blues\", values_format='d')\n",
        "    axes[idx, 1].set_title(f\"{method} - Confusion Matrix\")\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hw2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}