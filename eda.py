# -*- coding: utf-8 -*-
"""COMP9417_EDA_Submission

Automatically generated by Colab.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from scipy.stats import zscore, kstest, norm, ttest_rel, normaltest, levene
from xgboost import XGBClassifier
from sklearn.inspection import permutation_importance
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, LabelEncoder, LabelBinarizer
from sklearn.metrics import log_loss, accuracy_score, confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from imblearn.ensemble import BalancedRandomForestClassifier
from sklearn.manifold import TSNE
from sklearn.feature_selection import mutual_info_classif
from statsmodels.stats.outliers_influence import variance_inflation_factor
from collections import Counter

def weighted_log_loss(y_true, y_pred_proba, classes_):
    lb = LabelBinarizer()
    lb.fit(classes_)
    y_true_onehot = lb.transform(y_true)

    class_counts = np.sum(y_true_onehot, axis=0)
    class_weights = 1.0 / class_counts
    class_weights /= np.sum(class_weights)

    sample_weights = np.sum(y_true_onehot * class_weights, axis=1)
    loss = -np.mean(sample_weights * np.sum(y_true_onehot * np.log(y_pred_proba + 1e-15), axis=1))
    return loss

def weighted_log_loss_scorer(estimator, X, y):
    y_pred_proba = estimator.predict_proba(X)
    return -weighted_log_loss(y, y_pred_proba, estimator.classes_)

sns.set_style()

X = pd.read_csv("X_train.csv")
y = pd.read_csv("y_train.csv").squeeze()

print(X.describe())

"""**Basic Integrity Checks**"""

print((X.isnull().sum()).sum())
print((y.isnull().sum()).sum())

data = X.copy()
data['target'] = y

duplicates = data.duplicated()
print(duplicates.sum())

"""There are no immediate issues, thus we split our data"""

X_copy, y_copy = X.copy(), y.copy()

X, X_test, y, y_test = train_test_split(
    X, y, test_size=0.25, random_state=0, stratify=y
)

"""**Basic Analysis of Data**

First, we want to look at the distribution of the mean and standard deviation of our covariates.
"""

stats_df = pd.DataFrame({
    'Feature': X.select_dtypes(include=[np.number]).columns,
    'Mean': X.select_dtypes(include=[np.number]).mean().values,
    'Std': X.select_dtypes(include=[np.number]).std().values
})

fig_mean = px.histogram(stats_df, x='Mean', nbins=30, title='Distribution of Feature Means')
fig_mean.update_xaxes(dtick=0.01)
fig_mean.show()

fig_std = px.histogram(stats_df, x='Std', nbins=30, title='Distribution of Feature Standard Deviations')
fig_mean.update_xaxes(dtick=0.01)
fig_std.show()

"""This looks like its centered around mean 0 and std 1. Let's see if the data conforms to a normal distribution using a Person's test"""

normal_features = [
    column for column in X.columns
    if normaltest(X[column])[1] > 0.05
]

print(normal_features)

X_clipped = X.copy()

for column in X_clipped.columns:
    mean = X_clipped[column].mean()
    std = X_clipped[column].std()
    lower_bound = mean - 2.5 * std
    upper_bound = mean + 2.5 * std
    X_clipped[column] = np.clip(X_clipped[column], lower_bound, upper_bound)

normal_features_clipped = [
    column for column in X_clipped.columns
    if normaltest(X_clipped[column])[1] > 0.05
]

print(normal_features_clipped)
print(len(normal_features_clipped))

"""Next, let's look at the number of outliers"""

z_scores = np.abs(zscore(X))
mask = (z_scores > 3).any(axis=1)
print(mask.sum())

z_scores = np.abs(zscore(X))

threshold = 3
mask = np.abs(z_scores) > threshold
z_outlier_counts = pd.Series(mask.sum(axis=0), index=X.columns).sort_values(ascending=False)

fig = px.bar(
    x=z_outlier_counts.index.astype(str),
    y=z_outlier_counts.values,
    labels={'x': 'Feature', 'y': 'Number of Outliers'},
    title='Distribution of Outliers by Feature'
)

fig.update_layout(xaxis_tickangle=-45)
fig.show()

"""Above, we see that there are < 100 outliers per covariate. This is less than 0.1% of the data. Therefore, it is likely they do not have a singificant impact. We can check this by seeing how they affect our mean and deviation."""

z_scores = np.abs(zscore(X))
outlier_mask = (z_scores > 3).any(axis=1)

X_with_outliers = X.copy()
X_without_outliers = X.loc[~outlier_mask]

mean_with_outliers = X_with_outliers.mean()
mean_without_outliers = X_without_outliers.mean()

t_statistic, p_values = ttest_rel(mean_with_outliers, mean_without_outliers)

results_df = pd.DataFrame({
    'Feature': X.columns,
    'Mean_With_Outliers': mean_with_outliers.values,
    'Mean_Without_Outliers': mean_without_outliers.values,
    't_statistic': t_statistic,
    'p_value': p_values
})

significant_results = results_df[results_df['p_value'] < 0.05]
print(len(significant_results))

z_scores = np.abs(zscore(X))
outlier_mask = (z_scores > 3).any(axis=1)

X_with_outliers = X.copy()
X_without_outliers = X.loc[~outlier_mask]

levene_results = []

for feature in X.columns:
    stat, p_value = levene(X_with_outliers[feature], X_without_outliers[feature])
    levene_results.append((feature, stat, p_value))

levene_df = pd.DataFrame(levene_results, columns=['Feature', 'Levene_statistic', 'p_value'])

significant_variance_changes = levene_df[levene_df['p_value'] < 0.05]

print(len(significant_variance_changes))

fig = px.line(
    significant_variance_changes,
    y='Levene_statistic',
    title='Levene Statistic by Feature',
    width=800,
    height=400
)

fig.update_layout(
    showlegend=False,
    plot_bgcolor='white',
    xaxis=dict(showgrid=False, zeroline=False),
    yaxis=dict(showgrid=False, zeroline=False)
)

fig.show()

"""These showcase that the outliers do not have a statistically singificant impact on our feature means, but they do on our feature variances. Next, we will look at the class imbalance in our data.

**Data Imbalance**
"""

class_counts = y.value_counts().sort_values(ascending=False).reset_index()
class_counts.columns = ['Class', 'Count']
class_counts['Class'] = class_counts['Class'].astype(str)

fig = px.bar(class_counts, x='Class', y='Count', text='Count',
             title='Class Distribution in y',
             labels={'Class': 'Class', 'Count': 'Count'})

fig.update_traces(textposition='outside')
fig.show()

"""We can train a naive model to see how the data imbalance impacts performance"""

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.25, random_state=0, stratify=y
)

model = RandomForestClassifier(
    random_state=0,
    n_jobs=-1,
    max_depth=8
)
model.fit(X_train, y_train)

y_train_pred_proba = model.predict_proba(X_train)
y_val_pred_proba = model.predict_proba(X_val)

train_loss = log_loss(y_train, y_train_pred_proba)
val_loss = log_loss(y_val, y_val_pred_proba)

print(train_loss)
print(val_loss)

y_train_pred = model.predict(X_train)
y_val_pred = model.predict(X_val)

train_accuracy = accuracy_score(y_train, y_train_pred)
val_accuracy = accuracy_score(y_val, y_val_pred)

print(train_accuracy)
print(val_accuracy)



conf_matrix_train = confusion_matrix(y_train, y_train_pred)
conf_matrix_val = confusion_matrix(y_val, y_val_pred)

fig, axes = plt.subplots(1, 2, figsize=(20, 10))

sns.heatmap(conf_matrix_train, annot=True, fmt="d", cmap="Blues", ax=axes[0])
axes[0].set_title("Training Set Confusion Matrix")
axes[0].set_xlabel("Predicted Labels")
axes[0].set_ylabel("True Labels")

sns.heatmap(conf_matrix_val, annot=True, fmt="d", cmap="Greens", ax=axes[1])
axes[1].set_title("Validation Set Confusion Matrix")
axes[1].set_xlabel("Predicted Labels")
axes[1].set_ylabel("True Labels")

plt.tight_layout()
plt.show()

report_train_dict = classification_report(y_train, y_train_pred, output_dict=True)
report_val_dict = classification_report(y_val, y_val_pred, output_dict=True)

report_train_df = pd.DataFrame(report_train_dict).transpose().round(2)
report_val_df = pd.DataFrame(report_val_dict).transpose().round(2)

metrics = ['precision', 'recall', 'f1-score']

fig, axes = plt.subplots(1, 2, figsize=(20, 10))

sns.heatmap(report_train_df.loc[report_train_df.index[:-3], metrics], annot=True, cmap="Blues", fmt=".2f", ax=axes[0])
axes[0].set_title("Training Set Classification Report")
axes[0].set_xlabel("Metrics")
axes[0].set_ylabel("Classes")

sns.heatmap(report_val_df.loc[report_val_df.index[:-3], metrics], annot=True, cmap="Greens", fmt=".2f", ax=axes[1])
axes[1].set_title("Validation Set Classification Report")
axes[1].set_xlabel("Metrics")
axes[1].set_ylabel("Classes")

plt.tight_layout()
plt.show()

"""**Relationships in our Data**

First, we look at feature-feature relationships. Namely, we are interested in correlation and interaction between covariates
"""

def print_high_corr_pairs(X, threshold):
    corr_matrix = X.corr()
    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

    high_corr_pairs = upper_triangle.stack().reset_index()
    high_corr_pairs.columns = ['Feature 1', 'Feature 2', 'Correlation']
    high_corr_pairs = high_corr_pairs[high_corr_pairs['Correlation'].abs() > threshold]

    for _, row in high_corr_pairs.iterrows():
        print(f"({row['Feature 1']}, {row['Feature 2']}) = {row['Correlation']:.2f}")

    print(len(high_corr_pairs))

    return high_corr_pairs

"""First, we can take a look at pairs of covariates with a high correlation"""

print_high_corr_pairs(X, 0.7)

print_high_corr_pairs(X, 0.6)

# print_high_corr_pairs(X, 0.4)

"""There are 8 pairs with a Pearson's correlation above 0.6. To look at rank correlation, we can also print the pairs with a Spearman's correlation above 0.6"""

def print_high_spearmann_corr_pairs(X, threshold):
    corr_matrix = X.corr(method='spearman')
    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

    high_corr_pairs = upper_triangle.stack().reset_index()
    high_corr_pairs.columns = ['Feature 1', 'Feature 2', 'Correlation']
    high_corr_pairs = high_corr_pairs[high_corr_pairs['Correlation'].abs() > threshold]

    for _, row in high_corr_pairs.iterrows():
        print(f"({row['Feature 1']}, {row['Feature 2']}) = {row['Correlation']:.2f}")

    print(len(high_corr_pairs))

    return high_corr_pairs

print_high_spearmann_corr_pairs(X, 0.7)

print_high_spearmann_corr_pairs(X, 0.6)

print_high_spearmann_corr_pairs(X, 0.6)

"""We see that the correlation is relatively low in our data

We can also evaluate redundancy in our data using PCA.
"""

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_scaled = np.clip(X_scaled, -3, 3)

pca = PCA()
X_pca = pca.fit_transform(X_scaled)

cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)

threshold = 0.95
n_components_95 = np.argmax(cumulative_explained_variance >= threshold) + 1
variance_at_95 = cumulative_explained_variance[n_components_95 - 1]

fig = go.Figure()

fig.add_trace(go.Scatter(
    x=np.arange(1, len(cumulative_explained_variance) + 1),
    y=cumulative_explained_variance,
    mode='lines+markers',
    name='Cumulative Explained Variance'
))

fig.add_trace(go.Scatter(
    x=[n_components_95],
    y=[variance_at_95],
    mode='markers+text',
    name='95% Variance',
    text=[f'{n_components_95} components'],
    textposition='top center',
    marker=dict(size=10, symbol='circle', color='red')
))

fig.update_layout(
    title='Cumulative Explained Variance by Number of Principal Components',
    xaxis_title='Number of Principal Components',
    yaxis_title='Cumulative Explained Variance',
    yaxis=dict(range=[0, 1.05]),
    template='plotly_white'
)

fig.show()

"""**Feature-Response Relationships**

Next, we want to analyse the importance of each feature using permutation importance. It is important to note and discuss that we do not have any covariates with high correlation. They are all < 0.7
"""

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)

brf = BalancedRandomForestClassifier(
    n_estimators=100,
    criterion='entropy',
    max_depth=7,
    min_samples_leaf=1,
    min_samples_split=3,
    class_weight='balanced_subsample',
    oob_score=False,
    random_state=0,
    n_jobs=-1
)

brf.fit(X_train, y_train)

result = permutation_importance(
    brf,
    X_val,
    y_val,
    scoring=weighted_log_loss_scorer,
    n_repeats=10,
    random_state=0,
    n_jobs=-1
)

importances = result.importances_mean
std = result.importances_std
indices = np.argsort(importances)[::-1]

feature_names = [str(i) for i in range(X.shape[1])]

sorted_indices = indices
sorted_importances = importances[sorted_indices]
sorted_std = std[sorted_indices]
sorted_features = [feature_names[i] for i in sorted_indices]

fig = go.Figure()

fig.add_trace(go.Bar(
    x=sorted_features[:50],
    y=sorted_importances[:50],
    error_y=dict(type='data', array=sorted_std[:50]),
    name='Mean Importance',
    marker=dict(opacity=0.7)
))

fig.update_layout(
    title='Top 50 Features by Permutation Importance',
    xaxis_title='Feature',
    yaxis_title='Mean Decrease in Accuracy',
    xaxis_tickangle=45,
    template='plotly_white',
    showlegend=False
)

fig.show()

feature_names = [str(i) for i in range(X.shape[1])]

sorted_indices = indices
sorted_importances = importances[sorted_indices]
sorted_std = std[sorted_indices]
sorted_features = [feature_names[i] for i in sorted_indices]

fig = go.Figure()

fig.add_trace(go.Bar(
    x=sorted_features,
    y=sorted_importances,
    error_y=dict(type='data', array=sorted_std),
    name='Mean Importance',
    marker=dict(opacity=0.7)
))

fig.update_layout(
    title='Permutation Importance for All Features',
    xaxis_title='Feature Index',
    yaxis_title='Mean Decrease in Accuracy',
    xaxis_tickangle=45,
    template='plotly_white',
    showlegend=False,
    width=1400,
    height=1000,
    margin=dict(l=50, r=50, t=80, b=150)
)

fig.show()

num_negative = len(np.where(importances < 0)[0])
negative_imps = [str(i) for i in np.where(importances < 0)[0]]
print(num_negative)
print(negative_imps)

"""Finally, we want to test the relationships between X and specific classes in y. We can do this using a one-vs-rest (OVR) model such as the OVR logistic regression."""

scaler = StandardScaler()
scaler.fit(X_train)

X_train_scaled = scaler.transform(X_train)
X_val_scaled = scaler.transform(X_val)

model = LogisticRegression(
    penalty=None,
    multi_class='ovr',
    solver='lbfgs',
    max_iter=500,
    random_state=0,
    class_weight='balanced'
)
model.fit(X_train_scaled, y_train)

features = [213, 143, 256, 255, 214]
feature_freq = {feature: [] for feature in features}

coef_matrix = model.coef_

for i, class_coefs in enumerate(coef_matrix):
    top_5_indices = np.argsort(np.abs(class_coefs))[-5:]

    for feature in features:
        if feature in top_5_indices:
            feature_freq[feature].append(i)

summary = pd.DataFrame({
    'Feature': feature_freq.keys(),
    'Frequency': [len(classes) for classes in feature_freq.values()],
    'Classes': [classes for classes in feature_freq.values()]
})

summary.head()

top_features_all_classes = []

coef_matrix = model.coef_

for class_coefs in coef_matrix:
    top_5_indices = np.argsort(np.abs(class_coefs))[-5:]
    top_features_all_classes.extend(top_5_indices)

feature_counter = Counter(top_features_all_classes)

feature_count_df = pd.DataFrame({
    'Feature': [str(k) for k in feature_counter.keys()],
    'Count': list(feature_counter.values())
}).sort_values(by='Count', ascending=False)

fig = px.bar(
    feature_count_df,
    x='Feature',
    y='Count',
    title='Frequency of Features Appearing in Top 5 Across Classes',
    labels={'Feature': 'Feature Index', 'Count': 'Frequency'},
)

fig.update_layout(
    xaxis_tickangle=45,
    width=1000,
    height=500,
    template='plotly_white'
)

fig.show()

plot_classes = [1, 2, 16, 22]

coef_matrix = model.coef_

fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=[f"Class {cls}" for cls in plot_classes],
    horizontal_spacing=0.15,
    vertical_spacing=0.2
)

for i, class_idx in enumerate(plot_classes):
    class_coefs = coef_matrix[class_idx]

    top_5_indices = np.argsort(np.abs(class_coefs))[-5:][::-1]
    top_5_abs_coefs = np.abs(class_coefs[top_5_indices])

    row = i // 2 + 1
    col = i % 2 + 1

    fig.add_trace(
        go.Bar(
            x=[str(idx) for idx in top_5_indices],
            y=top_5_abs_coefs,
            text=[f"{val:.3f}" for val in top_5_abs_coefs],
        ),
        row=row, col=col
    )

fig.update_layout(
    height=700, width=900,
    title_text="Top 5 Features by Absolute Coefficient for Minority Classes",
    showlegend=False,
    template="plotly_white"
)

fig.update_xaxes(title_text="Feature Index")
fig.update_yaxes(title_text="|Coefficient|")

fig.show()

